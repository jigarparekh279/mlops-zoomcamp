{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee32dd39",
   "metadata": {},
   "source": [
    "## Most Important Steps\n",
    "\n",
    "Full play list -> [YouTube](https://www.youtube.com/playlist?list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d76f8",
   "metadata": {},
   "source": [
    "### 1.a Environment preparation\n",
    "\n",
    "Steps [here](https://youtu.be/IXSiYkP23zo?list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c3e5d9",
   "metadata": {},
   "source": [
    "(Optional if you already have a good enough local machine or in-house cluster account)\n",
    "\n",
    "- Launch a [EC2 machine on AWS](https://eu-central-1.console.aws.amazon.com/ec2/home?region=eu-central-1#Home:) --> see steps\n",
    "\n",
    "- Now ssh to the EC2 machine (better add it to the `.ssh/config` file) --> see steps\n",
    "\n",
    "    NOTE: The error msg while accessing EC2 machine means SSH refuses to use your `pem` key because it’s not private enough. Right now the .pem file has permissions 0664 → readable by you, your group, and others. That’s insecure, since anyone else on the system could copy your private key. Fix -\n",
    "\n",
    "    ```sh\n",
    "    chmod 600 /home/jigar/.ssh/mlops-zoomcamp.pem\n",
    "    ```\n",
    "\n",
    "    NOTE: after adding this EC2 machine to ssh config file we can simple do -\n",
    "\n",
    "    ```sh\n",
    "    ssh ec2-mlops-zoomcamp\n",
    "    ```\n",
    "\n",
    "- For IDE experience SSH connect via VSCode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d004cc4",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------\n",
    "- A little _detour_ to make the EC2 terminal pretty, convenient and development ready\n",
    "\n",
    "    ```sh\n",
    "    sudo apt update\n",
    "    # install zsh git curl\n",
    "    sudo apt install -y zsh git curl \n",
    "    sh -c \"$(wget -O- https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n",
    "    ```\n",
    "\n",
    "    Type 'y' when asked if you want zsh your default shell\n",
    "\n",
    "    Install more packages as needed e.g.\n",
    "\n",
    "    ```sh\n",
    "    sudo apt install -y tree\n",
    "    # usage\n",
    "    tree -L 3\n",
    "    ```\n",
    "\n",
    "    Add useful aliases e.g.\n",
    "\n",
    "    ```sh\n",
    "    echo \"alias cls='clear; ls -l --color=auto'\" >> ~/.zshrc && source ~/.zshrc\n",
    "    ``\n",
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6d9af2",
   "metadata": {},
   "source": [
    "- This EC2 machine comes with very latest version of python. But we want to use and older version that works with other libraries we intent to use. For this reason we download (using wget from [here](https://www.anaconda.com/download/success)) and install anaconda to create an env wth desired python version and libraries. --> see steps\n",
    "\n",
    "    ```sh\n",
    "    mkdir software; cd software; mkdir anaconda\n",
    "    wget https://repo.anaconda.com/archive/Anaconda3-2025.06-0-Linux-x86_64.sh\n",
    "    bash Anaconda3-2025.06-0-Linux-x86_64.sh\n",
    "    ```\n",
    "\n",
    "    - Specify the installation path as `/home/ubuntu/software/anaconda/anaconda3`\n",
    "\n",
    "    - Enter `yes` when prompted  with -\n",
    "\n",
    "        ```\n",
    "        Do you wish to update your shell profile to automatically initialize conda?\n",
    "        .\n",
    "        .\n",
    "        You can undo this by running `conda init --reverse $SHELL`? [yes|no]\n",
    "        [no] >>>\n",
    "        ```\n",
    "\n",
    "    - This initializes conda env every time we log in (notice `(base) ➜  ~` every new log in). For more details on how this works see ~./zshrc line starting with  `# >>> conda initialize >>>`\n",
    "\n",
    "    Create and activate a new conda env -\n",
    "\n",
    "    ```sh\n",
    "    conda create -n mlops-zoomcamp-env python==3.13.5\n",
    "    conda activate mlops-zoomcamp-env\n",
    "    \n",
    "    # (Optinal) if you have a requirements.txt file\n",
    "    pip install -r requirements\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984c1d12",
   "metadata": {},
   "source": [
    "- We now install docker and docker compose as we will need it later for containerization --> see steps\n",
    "\n",
    "    ```sh\n",
    "    sudo apt update\n",
    "    sudo apt install docker.io\n",
    "\n",
    "    cd /home/ubuntu/software/\n",
    "    mkdir docker\n",
    "    wget https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o docker-compose\n",
    "    chmod +x docker-compose\n",
    "    ```\n",
    "    Add path to docker compose to $PATH\n",
    "\n",
    "    ```sh\n",
    "    echo \"\\n# Path to docker\" >> ~/.zshrc && source ~/.zshrc\n",
    "    echo 'export PATH=\"${HOME}/software/docker:${PATH}\"' >> ~/.zshrc && source ~/.zshrc\n",
    "    ```\n",
    "\n",
    "- To run docker we need to use `sudo` everytime :(. To avoid this we need to our user to _docker group_\n",
    "\n",
    "    ```sh\n",
    "    sudo groupadd docker # group 'docker' already exists\n",
    "    sudo usermod -aG docker $USER # we add ourselves to user group of docker\n",
    "    ```\n",
    "\n",
    "    Then log out of EC2 machine and log back, and test using `docker run hello-world:latest` --> works w/o sudo! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d6902",
   "metadata": {},
   "source": [
    "### 1.b MLOps Maturity Model (see [Microsoft Guide](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/mlops-maturity-model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-18_12-17.png\" alt=\"alt text\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7731fe53",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-18_12-22.png\" alt=\"alt text\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-18_12-24.png\" alt=\"alt text\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0a0f8",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-18_12-28.png\" alt=\"alt text\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47644a3c",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-18_12-31.png\" alt=\"alt text\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Expreiment Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbfdfcf",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-18_16-47.png\" alt=\"alt text\" width=\"350\"/>\n",
    "\n",
    "<img src=\"2025-09-18_16-49.png\" alt=\"alt text\" width=\"400\"/>\n",
    "\n",
    "<img src=\"2025-09-18_16-51.png\" alt=\"alt text\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLFlow backend settings in terminal\n",
    "\n",
    "```sh\n",
    "# Tell mlflow to store artifacts and metadata in sqlite\n",
    "mlflow ui --backend-store-uri sqlite:///mlflow.db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d0b9d0",
   "metadata": {},
   "source": [
    "MLFlow import in python\n",
    "\n",
    "```py\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")  # set backend to the same as above \n",
    "mlflow.set_experiment(\"nyc-taxi-experiment\")    # name of ml experiment\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f34a98",
   "metadata": {},
   "source": [
    "---\n",
    "Tracking a simple experiment\n",
    "\n",
    "```py\n",
    "with mlflow.start_run():\n",
    "\n",
    "    mlflow.set_tag(\"developer\", \"jigar\")    # e.g.\n",
    "\n",
    "    alpha = 0.1\n",
    "    mlflow.log_param(\"alpha\", alpha)        # a good param to log    \n",
    "    lr = Lasso(alpha)\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lr.predict(X_val)\n",
    "    rmse = root_mean_squared_error(y_val, y_pred)\n",
    "    mlflow.log_metric(\"rmse\", rmse)         # a good metric to log\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a393691",
   "metadata": {},
   "source": [
    "---\n",
    "Tracking an experiment during hyper-parameter optimization - the objective func looks somewhat like this -\n",
    "\n",
    "```py\n",
    "def objective(params):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"developer\", \"jigar\")    # good to know who wrote this code\n",
    "        mlflow.set_tag(\"model\", \"xgboost\")      # model name\n",
    "        mlflow.log_params(params)\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=train,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(valid, 'validation')],\n",
    "            early_stopping_rounds=50\n",
    "        )\n",
    "        y_pred = booster.predict(valid)\n",
    "        rmse = root_mean_squared_error(y_val, y_pred)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    return {'loss': rmse, 'status': STATUS_OK}  # a good metric to log\n",
    "```\n",
    "\n",
    "\n",
    "Typical search space may look like -\n",
    "\n",
    "```py\n",
    "search_space = {\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "    'objective': 'reg:linear',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "best_result = fmin(\n",
    "    fn=objective, space=search_space, algo=tpe.suggest, \n",
    "    max_evals=50, trials=Trials()\n",
    ")\n",
    "```\n",
    "\n",
    "Check out `hyperopt` python library to relate with above \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4c190e",
   "metadata": {},
   "source": [
    "---\n",
    "Auto logging is a powerful feature that allows you to log metrics, parameters, and models without the need for explicit log statements. All you need to do is to call `mlflow.autolog()` (see [here](https://mlflow.org/docs/latest/ml/tracking/autolog/)) before your training code. \n",
    "\n",
    "```py\n",
    "mlflow.autolog()\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "mlflow.xgboost.autolog()\n",
    "mlflow.xgboost.autolog(disable=True)\n",
    "\n",
    "mlflow.pytorch.autolog()  # also works with Lightning\n",
    "mlflow.pytorch.autolog(disable=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d402fc",
   "metadata": {},
   "source": [
    "### 2b. Model Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5990e0",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-19_13-42.png\" alt=\"alt text\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8852fcf",
   "metadata": {},
   "source": [
    "We already saw the experiment tracting above. Now lets see how can we use mlflow for model saving and versioning.\n",
    "\n",
    "```py\n",
    "mlflow.sklearn.log_model(preprocessor, name=\"preprocessor\")         # log a preprocessor\n",
    "mlflow.sklearn.log_model(clf, name=\"model_name\")        # log a model\n",
    "```\n",
    "\n",
    "So now we can simply download this model (with its preprocessor) and run (or even deploy).\n",
    "\n",
    "```py\n",
    "preprocessor_uri = f\"runs:/<preprocessor_run_id>/preprocessor\"\n",
    "model_uri = f\"runs:/<model_run_id>/rf_model\"\n",
    "\n",
    "loaded_preprocessor = mlflow.sklearn.load_model(preprocessor_uri)\n",
    "loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "X_test_scaled = loaded_preprocessor.transform(X_test)\n",
    "preds = loaded_model.predict(X_test_scaled)\n",
    "```\n",
    "\n",
    "NOTE - even a complete pipeline (preprocessor + model + postprocessor) can be logged and loaded!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08497919",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-19_15-20.png\" alt=\"alt text\" width=\"550\"/>\n",
    "<img src=\"2025-09-19_15-16.png\" alt=\"alt text\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d97414",
   "metadata": {},
   "source": [
    "### 2c. Model Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd7381e",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-19_17-06.png\" alt=\"alt text\" width=\"550\"/>\n",
    "\n",
    "Model Registry contains all the models which are ready for production (job of Data Scientist to furnish these models). \n",
    "\n",
    "* It doesn't involve deployment - but only lists the production ready models -- with stages as \"labels\" assinged to the models! \n",
    "\n",
    "    For example: \n",
    "    - models ready for production (v3, v4) --> staging stage (`@challenger`)\n",
    "    - models in production (v2) --> production stage (`@champion`)\n",
    "    - models to be archived for some reason (v1) --> archive stage (`@retired`)\n",
    "    - if neeed v1 can be rolled back to deployment (back to production stage)\n",
    "\n",
    "* The deployment engineer can have a look here to figure out -- what are the hyper-parameters used, size of the model, performance etc. -- based on that they can decide to move the model between the different stages in model registry\n",
    "\n",
    "* Model Registry needs to be complemented with some CI/CD code in order to perform actual _deployement_ of those models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea8636",
   "metadata": {},
   "source": [
    "After performing extensive model experimentation, to choose the model to be pushed into model registry we need to focus on a few imp detailed of best performing models e.g. run time (training & inference), model size, performance metric, etc. -- watch this section [here](https://youtu.be/TKHU7HAvGH8?list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK&t=310)\n",
    "\n",
    "<img src=\"2025-09-19_18-37.png\" alt=\"alt text\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c12926",
   "metadata": {},
   "source": [
    "We can also check our ml experiments using python.\n",
    "\n",
    "```py\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "MLFLOW_TRACKING_URI = \"sqlite:///mlflow.db\"\n",
    "client = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\n",
    "\n",
    "# Gather top 3 runs in exp with id 1\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=\"1\",\n",
    "    filter_string=\"metrics.rmse < 5.63\",    # use tags, metrics or params to filter\n",
    "    run_view_type=ViewType.ACTIVE_ONLY,\n",
    "    max_results=5,\n",
    "    order_by=[\"metrics.rmse ASC\"]           # use metric to arrange the runs in some order\n",
    ")\n",
    "\n",
    "# Print id and metric\n",
    "for run in runs:\n",
    "    print(f\"run id: {run.info.run_id}, rmse: {run.data.metrics['rmse']:.4f}\")\n",
    "\n",
    "# Register a new version of a model\n",
    "import mlflow\n",
    "model_name = \"nyc-taxi-regressor\"\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "run_id = \"f933c26b98914d81a9ec2b0f9bf4551e\"     # a model run_id that was not registered yet\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "mlflow.register_model(model_uri=model_uri, name=model_name)\n",
    "\n",
    "# # Transition model from one tag or alias (stage) to another\n",
    "# client.set_registered_model_alias(name=model_name, alias=\"hopeful\", version=4)\n",
    "# client.set_model_version_tag(name=model_name, key=\"model\", value=\"etr\", version=4)\n",
    "\n",
    "# Or transition model stage (old mlflow model registry)\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name, \n",
    "    version=4, \n",
    "    stage=\"Staging\", \n",
    "    archive_existing_versions=False\n",
    ")\n",
    "\n",
    "# Add description\n",
    "from datetime import datetime\n",
    "date = datetime.today().date()\n",
    "client.update_model_version(\n",
    "    name=model_name, version=4, \n",
    "    description=f\"This model was transitioned to Staging on {date}\"\n",
    ")\n",
    "\n",
    "# Assume version 4 (latest) performs better than version 1 (current prod version)\n",
    "# So we push version 4 to production stage and the existing prod version to archive\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name, \n",
    "    version=4, \n",
    "    stage=\"Production\", \n",
    "    archive_existing_versions=True\n",
    ")\n",
    "client.update_model_version(\n",
    "    name=model_name,\n",
    "    version=4,\n",
    "    description=f\"This model was transitioned to Production on {date}\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e65b4",
   "metadata": {},
   "source": [
    "### 2d. MLflow in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ccc040",
   "metadata": {},
   "source": [
    "Just follow this [video](https://youtu.be/1ykg4YmbFVA?list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK) and the notebooks for each scenario.\n",
    "\n",
    "<img src=\"2025-09-23_12-51.png\" alt=\"alt text\" width=\"500\"/>\n",
    "<img src=\"2025-09-23_12-54.png\" alt=\"alt text\" width=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario 1: A single data scientist participating in an ML competition\n",
    "\n",
    "MLflow setup:\n",
    "* Tracking server: no\n",
    "* Backend store: local filesystem\n",
    "* Artifacts store: local filesystem\n",
    "\n",
    "The experiments can be explored locally by launching the MLflow UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be5f9f8",
   "metadata": {},
   "source": [
    "```py\n",
    "# mlflow assumes this dir to save exps\n",
    "print(f\"tracking URI: '{mlflow.get_tracking_uri()}'\")  \n",
    "# creates mlruns dir with 0 dir with a yaml file for 'Default' exp name\n",
    "mlflow.search_experiments()\n",
    "# creates a new exp\n",
    "mlflow.set_experiment(\"my-experiment-1\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Model training ...\n",
    "    ...\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_score(y, y_pred))\n",
    "    mlflow.sklearn.log_model(lr, artifact_path=\"models\")\n",
    "    print(f\"default artifacts URI: '{mlflow.get_artifact_uri()}'\")\n",
    "```\n",
    "\n",
    "Have a look at the `meta.yaml` \n",
    "```yaml\n",
    "artifact_location: file:///home/jigar/DataTalksClub/mlops-zoomcamp/02-experiment-tracking/running-mlflow-examples/mlruns/844070181241854736\n",
    "creation_time: 1758625632232\n",
    "experiment_id: '844070181241854736'\n",
    "last_update_time: 1758625632232\n",
    "lifecycle_stage: active\n",
    "name: my-experiment-1\n",
    "```\n",
    "\n",
    "and the dir with artifacts/models, metrics, params, tags, and another meta.yaml. As we run more exps under the same exp name we will have more of such dirs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79b4065",
   "metadata": {},
   "source": [
    "#### Scenario 2: A cross-functional team with one data scientist working on an ML model\n",
    "\n",
    "\n",
    "MLflow setup:\n",
    "- tracking server: yes, local server\n",
    "- backend store: sqlite database\n",
    "- artifacts store: local filesystem\n",
    "\n",
    "The experiments can be explored locally by accessing the local tracking server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to launch the mlflow server locally by running in the terminal:\n",
    "\n",
    "`mlflow server --backend-store-uri sqlite:///backend.db`\n",
    "\n",
    "This will store the meta data about the experiment in `backend.db`\n",
    "\n",
    "```py\n",
    "# We set the tracking uri to this server address instead of \n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "\n",
    "```\n",
    "\n",
    "Follow the previous steps -- now we do not see different _metadata_ dirs for models, metrics, params, tags and met.yaml file -- these are all stored in the `backend.db`\n",
    "\n",
    "Then you can register this model\n",
    "\n",
    "```py\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient(\"http://127.0.0.1:5000\")\n",
    "run_id = client.search_runs(experiment_ids='1')[0].info.run_id\n",
    "mlflow.register_model(\n",
    "    model_uri=f\"runs:/{run_id}/models\",\n",
    "    name='iris-classifier'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586b8edb",
   "metadata": {},
   "source": [
    "### Scenario 3: Multiple data scientists working on multiple ML models\n",
    "\n",
    "MLflow setup:\n",
    "* Tracking server: yes, remote server (EC2). (mlflow server similar to _localhost_)\n",
    "* Backend store: postgresql database. (stores metadata etc)\n",
    "* Artifacts store: s3 bucket. (stores models etc.)\n",
    "\n",
    "The experiments can be explored by accessing the remote server.\n",
    "\n",
    "The example uses AWS to host a remote server. In order to run the example you'll need an AWS account. Follow the steps described in the file `mlflow_on_aws.md` to create a new AWS account and launch the tracking server.\n",
    "\n",
    "Follow the steps [here](https://youtu.be/1ykg4YmbFVA?list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK&t=1379)\n",
    "\n",
    "```sh\n",
    "mlflow server -h 0.0.0.0 -p 5000 --backend-store-uri postgresql://mlflow:79PPBWqKgxxzxh28yXpQ@mlflow-database.cha2w240m3r7.eu-central-1.rds.amazonaws.com:5432/mlflow_db --default-artifact-root s3://mlflow-artifacts-remote-279\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25eb6a",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-23_14-38.png\" alt=\"alt text\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8a4fa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
