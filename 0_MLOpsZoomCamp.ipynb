{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee32dd39",
   "metadata": {},
   "source": [
    "## Most Important Steps\n",
    "\n",
    "Full play list -> [YouTube](https://www.youtube.com/playlist?list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d76f8",
   "metadata": {},
   "source": [
    "### 1.a Environment preparation\n",
    "\n",
    "Steps [here](https://youtu.be/IXSiYkP23zo?list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c3e5d9",
   "metadata": {},
   "source": [
    "(Optional if you already have a good enough local machine or in-house cluster account)\n",
    "\n",
    "- Launch a [EC2 machine on AWS](https://eu-central-1.console.aws.amazon.com/ec2/home?region=eu-central-1#Home:) --> see steps\n",
    "\n",
    "- Now ssh to the EC2 machine (better add it to the `.ssh/config` file) --> see steps\n",
    "\n",
    "    NOTE: The error msg while accessing EC2 machine means SSH refuses to use your `pem` key because it’s not private enough. Right now the .pem file has permissions 0664 → readable by you, your group, and others. That’s insecure, since anyone else on the system could copy your private key. Fix -\n",
    "\n",
    "    ```sh\n",
    "    chmod 600 /home/jigar/.ssh/mlops-zoomcamp.pem\n",
    "    ```\n",
    "\n",
    "    NOTE: after adding this EC2 machine to ssh config file we can simple do -\n",
    "\n",
    "    ```sh\n",
    "    ssh ec2-mlops-zoomcamp\n",
    "    ```\n",
    "\n",
    "- For IDE experience SSH connect via VSCode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d004cc4",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------\n",
    "- A little _detour_ to make the EC2 terminal pretty, convenient and development ready\n",
    "\n",
    "    ```sh\n",
    "    sudo apt update\n",
    "    # install zsh git curl\n",
    "    sudo apt install -y zsh git curl \n",
    "    sh -c \"$(wget -O- https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n",
    "    ```\n",
    "\n",
    "    Type 'y' when asked if you want zsh your default shell\n",
    "\n",
    "    Install more packages as needed e.g.\n",
    "\n",
    "    ```sh\n",
    "    sudo apt install -y tree\n",
    "    # usage\n",
    "    tree -L 3\n",
    "    ```\n",
    "\n",
    "    Add useful aliases e.g.\n",
    "\n",
    "    ```sh\n",
    "    echo \"alias cls='clear; ls -l --color=auto'\" >> ~/.zshrc && source ~/.zshrc\n",
    "    ``\n",
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6d9af2",
   "metadata": {},
   "source": [
    "- This EC2 machine comes with very latest version of python. But we want to use and older version that works with other libraries we intent to use. For this reason we download (using wget from [here](https://www.anaconda.com/download/success)) and install anaconda to create an env wth desired python version and libraries. --> see steps\n",
    "\n",
    "    ```sh\n",
    "    mkdir software; cd software; mkdir anaconda\n",
    "    wget https://repo.anaconda.com/archive/Anaconda3-2025.06-0-Linux-x86_64.sh\n",
    "    bash Anaconda3-2025.06-0-Linux-x86_64.sh\n",
    "    ```\n",
    "\n",
    "    - Specify the installation path as `/home/ubuntu/software/anaconda/anaconda3`\n",
    "\n",
    "    - Enter `yes` when prompted  with -\n",
    "\n",
    "        ```\n",
    "        Do you wish to update your shell profile to automatically initialize conda?\n",
    "        .\n",
    "        .\n",
    "        You can undo this by running `conda init --reverse $SHELL`? [yes|no]\n",
    "        [no] >>>\n",
    "        ```\n",
    "\n",
    "    - This initializes conda env every time we log in (notice `(base) ➜  ~` every new log in). For more details on how this works see ~./zshrc line starting with  `# >>> conda initialize >>>`\n",
    "\n",
    "    Create and activate a new conda env -\n",
    "\n",
    "    ```sh\n",
    "    conda create -n mlops-zoomcamp-env python==3.13.5\n",
    "    conda activate mlops-zoomcamp-env\n",
    "    \n",
    "    # (Optinal) if you have a requirements.txt file\n",
    "    pip install -r requirements\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984c1d12",
   "metadata": {},
   "source": [
    "- We now install docker and docker compose as we will need it later for containerization --> see steps\n",
    "\n",
    "    ```sh\n",
    "    sudo apt update\n",
    "    sudo apt install docker.io\n",
    "\n",
    "    cd /home/ubuntu/software/\n",
    "    mkdir docker\n",
    "    wget https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o docker-compose\n",
    "    chmod +x docker-compose\n",
    "    ```\n",
    "    Add path to docker compose to $PATH\n",
    "\n",
    "    ```sh\n",
    "    echo \"\\n# Path to docker\" >> ~/.zshrc && source ~/.zshrc\n",
    "    echo 'export PATH=\"${HOME}/software/docker:${PATH}\"' >> ~/.zshrc && source ~/.zshrc\n",
    "    ```\n",
    "\n",
    "- To run docker we need to use `sudo` everytime :(. To avoid this we need to our user to _docker group_\n",
    "\n",
    "    ```sh\n",
    "    sudo groupadd docker # group 'docker' already exists\n",
    "    sudo usermod -aG docker $USER # we add ourselves to user group of docker\n",
    "    ```\n",
    "\n",
    "    Then log out of EC2 machine and log back, and test using `docker run hello-world:latest` --> works w/o sudo! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d6902",
   "metadata": {},
   "source": [
    "### 1.b MLOps Maturity Model (see [Microsoft Guide](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/mlops-maturity-model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-18_12-17.png\" alt=\"alt text\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7731fe53",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-18_12-22.png\" alt=\"alt text\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-18_12-24.png\" alt=\"alt text\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0a0f8",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-18_12-28.png\" alt=\"alt text\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47644a3c",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-18_12-31.png\" alt=\"alt text\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Expreiment Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbfdfcf",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-18_16-47.png\" alt=\"alt text\" width=\"350\"/>\n",
    "\n",
    "<img src=\"2025-09-18_16-49.png\" alt=\"alt text\" width=\"400\"/>\n",
    "\n",
    "<img src=\"2025-09-18_16-51.png\" alt=\"alt text\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLFlow backend settings in terminal\n",
    "\n",
    "```sh\n",
    "# Tell mlflow to store artifacts and metadata in sqlite\n",
    "mlflow ui --backend-store-uri sqlite:///mlflow.db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d0b9d0",
   "metadata": {},
   "source": [
    "MLFlow import in python\n",
    "\n",
    "```py\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")  # set backend to the same as above \n",
    "mlflow.set_experiment(\"nyc-taxi-experiment\")    # name of ml experiment\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f34a98",
   "metadata": {},
   "source": [
    "---\n",
    "Tracking a simple experiment\n",
    "\n",
    "```py\n",
    "with mlflow.start_run():\n",
    "\n",
    "    mlflow.set_tag(\"developer\", \"jigar\")    # e.g.\n",
    "\n",
    "    alpha = 0.1\n",
    "    mlflow.log_param(\"alpha\", alpha)        # a good param to log    \n",
    "    lr = Lasso(alpha)\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lr.predict(X_val)\n",
    "    rmse = root_mean_squared_error(y_val, y_pred)\n",
    "    mlflow.log_metric(\"rmse\", rmse)         # a good metric to log\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a393691",
   "metadata": {},
   "source": [
    "---\n",
    "Tracking an experiment during hyper-parameter optimization - the objective func looks somewhat like this -\n",
    "\n",
    "```py\n",
    "def objective(params):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"developer\", \"jigar\")    # good to know who wrote this code\n",
    "        mlflow.set_tag(\"model\", \"xgboost\")      # model name\n",
    "        mlflow.log_params(params)\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=train,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(valid, 'validation')],\n",
    "            early_stopping_rounds=50\n",
    "        )\n",
    "        y_pred = booster.predict(valid)\n",
    "        rmse = root_mean_squared_error(y_val, y_pred)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    return {'loss': rmse, 'status': STATUS_OK}  # a good metric to log\n",
    "```\n",
    "\n",
    "\n",
    "Typical search space may look like -\n",
    "\n",
    "```py\n",
    "search_space = {\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "    'objective': 'reg:linear',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "best_result = fmin(\n",
    "    fn=objective, space=search_space, algo=tpe.suggest, \n",
    "    max_evals=50, trials=Trials()\n",
    ")\n",
    "```\n",
    "\n",
    "Check out `hyperopt` python library to relate with above \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4c190e",
   "metadata": {},
   "source": [
    "---\n",
    "Auto logging is a powerful feature that allows you to log metrics, parameters, and models without the need for explicit log statements. All you need to do is to call `mlflow.autolog()` (see [here](https://mlflow.org/docs/latest/ml/tracking/autolog/)) before your training code. \n",
    "\n",
    "```py\n",
    "mlflow.autolog()\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "mlflow.xgboost.autolog()\n",
    "mlflow.xgboost.autolog(disable=True)\n",
    "\n",
    "mlflow.pytorch.autolog()  # also works with Lightning\n",
    "mlflow.pytorch.autolog(disable=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d402fc",
   "metadata": {},
   "source": [
    "### 2b. Model Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5990e0",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-19_13-42.png\" alt=\"alt text\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8852fcf",
   "metadata": {},
   "source": [
    "We already saw the experiment tracting above. Now lets see how can we use mlflow for model saving and versioning.\n",
    "\n",
    "```py\n",
    "mlflow.sklearn.log_model(preprocessor, name=\"preprocessor\")         # log a preprocessor\n",
    "mlflow.sklearn.log_model(clf, name=\"model_name\")        # log a model\n",
    "```\n",
    "\n",
    "So now we can simply download this model (with its preprocessor) and run (or even deploy).\n",
    "\n",
    "```py\n",
    "preprocessor_uri = f\"runs:/<preprocessor_run_id>/preprocessor\"\n",
    "model_uri = f\"runs:/<model_run_id>/rf_model\"\n",
    "\n",
    "loaded_preprocessor = mlflow.sklearn.load_model(preprocessor_uri)\n",
    "loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "X_test_scaled = loaded_preprocessor.transform(X_test)\n",
    "preds = loaded_model.predict(X_test_scaled)\n",
    "```\n",
    "\n",
    "NOTE - even a complete pipeline (preprocessor + model + postprocessor) can be logged and loaded!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08497919",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-19_15-20.png\" alt=\"alt text\" width=\"550\"/>\n",
    "<img src=\"2025-09-19_15-16.png\" alt=\"alt text\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d97414",
   "metadata": {},
   "source": [
    "### 2c. Model Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd7381e",
   "metadata": {},
   "source": [
    "<img src=\"2025-09-19_17-06.png\" alt=\"alt text\" width=\"550\"/>\n",
    "\n",
    "Model Registry contains all the models which are ready for production (job of Data Scientist to furnish these models). \n",
    "\n",
    "* It doesn't involve deployment - but only lists the production ready models -- with stages as \"labels\" assinged to the models! \n",
    "\n",
    "    For example: \n",
    "    - models ready for production (v3, v4) --> staging stage (`@challenger`)\n",
    "    - models in production (v2) --> production stage (`@champion`)\n",
    "    - models to be archived for some reason (v1) --> archive stage (`@retired`)\n",
    "    - if neeed v1 can be rolled back to deployment (back to production stage)\n",
    "\n",
    "* The deployment engineer can have a look here to figure out -- what are the hyper-parameters used, size of the model, performance etc. -- based on that they can decide to move the model between the different stages in model registry\n",
    "\n",
    "* Model Registry needs to be complemented with some CI/CD code in order to perform actual _deployement_ of those models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea8636",
   "metadata": {},
   "source": [
    "After performing extensive model experimentation, to choose the model to be pushed into model registry we need to focus on a few imp detailed of best performing models e.g. run time (training & inference), model size, performance metric, etc. -- watch this section [here](https://youtu.be/TKHU7HAvGH8?list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK&t=310)\n",
    "\n",
    "<img src=\"2025-09-19_18-37.png\" alt=\"alt text\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c12926",
   "metadata": {},
   "source": [
    "We can also check our ml experiments using python.\n",
    "\n",
    "```py\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "MLFLOW_TRACKING_URI = \"sqlite:///mlflow.db\"\n",
    "client = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\n",
    "\n",
    "# Gather top 3 runs in exp with id 1\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=\"1\",\n",
    "    filter_string=\"metrics.rmse < 5.63\",\n",
    "    run_view_type=ViewType.ACTIVE_ONLY,\n",
    "    max_results=5,\n",
    "    order_by=[\"metrics.rmse ASC\"]\n",
    ")\n",
    "\n",
    "# Print id and metric\n",
    "for run in runs:\n",
    "    print(f\"run id: {run.info.run_id}, rmse: {run.data.metrics['rmse']:.4f}\")\n",
    "\n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e65b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fe8a4fa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
